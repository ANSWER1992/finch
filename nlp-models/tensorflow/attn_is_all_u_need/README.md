
>python train.py --hidden_units=128 --num_epochs=30 --num_heads=4 --positional_encoding=learned --tied_proj_weight --tied_embedding --activation=lrelu

The image below (a kind of) illustrates how an army of attentions work:
![alt text](https://github.com/zhedongzheng/finch/blob/master/assets/transform20fps.gif)
